{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "Go back to your code from the last lesson, where you built a simple first iteration of a POI identifier using a decision tree and one feature. Copy the POI identifier that you built into the skeleton code in evaluation/evaluate_poi_identifier.py. Recall that at the end of that project, your identifier had an accuracy (on the test set) of 0.724. Not too bad, right? Let’s dig into your predictions a little more carefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.724137931034\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Starter code for the evaluation mini-project.\n",
    "    Start by copying your trained/tested POI identifier from\n",
    "    that which you built in the validation mini-project.\n",
    "    This is the second step toward building your POI identifier!\n",
    "    Start by loading/formatting the data...\n",
    "\"\"\"\n",
    "\n",
    "import pickle\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "\n",
    "data_dict = pickle.load(open(\"final_project_dataset.pkl\", \"rb\") )\n",
    "\n",
    "### add more features to features_list!\n",
    "features_list = [\"poi\", \"salary\"]\n",
    "\n",
    "data = featureFormat(data_dict, features_list, sort_keys = 'python2_lesson14_keys.pkl')\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "### your code goes here\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create classifier\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "\n",
    "# Fit the classifier on the training features and labels\n",
    "clf.fit(features_train, labels_train)\n",
    "\n",
    "# Make prediction - Store predictions in a list named pred\n",
    "pred = clf.predict(features_test)\n",
    "\n",
    "# Calculate the accuracy on the test data\n",
    "print(clf.score(features_test, labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of POIs in Test Set\n",
    "How many POIs are predicted for the test set for your POI identifier?\n",
    "\n",
    "(Note that we said test set! We are not looking for the number of POIs in the whole dataset.)\n",
    "\n",
    "The line:\n",
    "\n",
    "data = featureFormat(data_dict, features_list)\n",
    "\n",
    "converts 'True' to 1 and 'False' to 0 for the 'poi' variable, so that should help you to determine the number of pois in 'labels_test' (as it will now contain zeros and ones)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "4.0\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "# poi is the placeholder for each item in 'labels_test'\n",
    "# 'labels_test' is the array\n",
    "for poi in labels_test:\n",
    "    if poi == 1:\n",
    "        count += 1\n",
    "print(count)\n",
    "\n",
    "# Alternative solution\n",
    "print(sum(labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of People in Test Set\n",
    "How many people total are in your test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n"
     ]
    }
   ],
   "source": [
    "print(len(labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy of a Biased Identifier\n",
    "If your identifier predicted 0 (not POI) for everyone in the test set, what would its accuracy be?\n",
    "*  The right answer is (29-4)/29 which is equal to 0.8621."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "0.862068965517\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics\n",
    "\n",
    "pred = [0.] * len(labels_test)\n",
    "\n",
    "print(labels_test)\n",
    "print(pred)\n",
    "print(sklearn.metrics.accuracy_score(labels_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This question was more to illustrate that if you train using a skewed training set you can still end up with a high accuracy and in more complex examples you might take high accuracy as good outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of True Positives\n",
    "Look at the predictions of your model and compare them to the true test labels. Do you get any true positives? (In this case, we define a true positive as a case where both the actual label and the predicted label are 1)\n",
    "* Nope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Correct Positive Predictions\n",
      "0.0\n",
      "\n",
      "[[25  0]\n",
      " [ 4  0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# A positive prediction differs from a 'correct positive prediction', i.e. a 'true positive'.\n",
    "# So, to find out how many positive predictions the code has made,\n",
    "# all that you need to do is to sum the pred data.\n",
    "\n",
    "# If you do want to find the number of 'correct positive predictions',\n",
    "# or 'true positives, you can zip labels and pred and use list comprehension:\n",
    "\n",
    "cpp = [1 for j in zip(labels, pred) if j[0] == j[1] and j[1] == 1]\n",
    "\n",
    "# then you can sum them, to find out how many correct predictions your code has made:\n",
    "\n",
    "no_cpp = np.sum(cpp)\n",
    "\n",
    "print(\"Number of Correct Positive Predictions\")\n",
    "print(no_cpp)\n",
    "print()\n",
    "print(sklearn.metrics.confusion_matrix(labels_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unpacking Into Precision and Recall\n",
    "As you may now see, having imbalanced classes like we have in the Enron dataset (many more non-POIs than POIs) introduces some special challenges, namely that you can just guess the more common class label for every point, not a very insightful strategy, and still get pretty good accuracy!\n",
    "\n",
    "Precision and recall can help illuminate your performance better. Use the precision_score and recall_score available in sklearn.metrics to compute those quantities.\n",
    "\n",
    "What’s the precision?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.724137931034\n",
      "Precision: 0.0\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(features, labels, test_size = 0.3, random_state=42)\n",
    "\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(x_train, y_train)\n",
    "y_pred = clf.predict(x_test)\n",
    "\n",
    "print(\"Accuracy:\", clf.score(x_test, y_test))\n",
    "\n",
    "print(\"Precision:\", sklearn.metrics.precision_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall of Your POI Identifier\n",
    "What’s the recall? \n",
    "\n",
    "(Note: you may see a message like UserWarning: The precision and recall are equal to zero for some labels. Just like the message says, there can be problems in computing other metrics (like the F1 score) when precision and/or recall are zero, and it wants to warn you when that happens.) \n",
    "\n",
    "Obviously this isn’t a very optimized machine learning strategy (we haven’t tried any algorithms besides the decision tree, or tuned any parameters, or done any feature selection), and now seeing the precision and recall should make that much more apparent than the accuracy did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Recall:\", sklearn.metrics.recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Many True Positives?\n",
    "In the final project you’ll work on optimizing your POI identifier, using many of the tools learned in this course. Hopefully one result will be that your precision and/or recall will go up, but then you’ll have to be able to interpret them. \n",
    "\n",
    "Here are some made-up predictions and true labels for a hypothetical test set; fill in the following boxes to practice identifying true positives, false positives, true negatives, and false negatives. Let’s use the convention that “1” signifies a positive result, and “0” a negative. \n",
    "\n",
    "predictions = [0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1] \n",
    "\n",
    "true labels = [0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0]\n",
    "\n",
    "How many true positives are there?\n",
    "\n",
    "* 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of True Positives\n",
      "6\n",
      "[[9 3]\n",
      " [2 6]]\n"
     ]
    }
   ],
   "source": [
    "predictions = [0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1]\n",
    "true_labels = [0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0]\n",
    "\n",
    "cpp = [1 for j in zip(true_labels, predictions) if j[0] == j[1] and j[1] == 1]\n",
    "no_cpp = np.sum(cpp)\n",
    "\n",
    "print(\"Number of True Positives\")\n",
    "print(no_cpp)\n",
    "\n",
    "# Alternative solution\n",
    "print(sklearn.metrics.confusion_matrix(true_labels, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Many True Negatives?\n",
    "How many true negatives are there in this example?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of True Negatives\n",
      "9\n",
      "[[9 3]\n",
      " [2 6]]\n"
     ]
    }
   ],
   "source": [
    "cpp = [1 for j in zip(true_labels, predictions) if j[0] == j[1] and j[1] == 0]\n",
    "no_cpp = np.sum(cpp)\n",
    "\n",
    "print(\"Number of True Negatives\")\n",
    "print(no_cpp)\n",
    "\n",
    "# Alternative solution\n",
    "print(sklearn.metrics.confusion_matrix(true_labels, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### False Positives?\n",
    "How many false positives are there?\n",
    "* 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### False Negatives?\n",
    "How many false negatives are there?\n",
    "* 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision\n",
    "What's the precision of this classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.666666666667\n"
     ]
    }
   ],
   "source": [
    "print(\"Precision:\", sklearn.metrics.precision_score(true_labels, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall\n",
    "What's the recall of this classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.75\n"
     ]
    }
   ],
   "source": [
    "print(\"Recall:\", sklearn.metrics.recall_score(true_labels, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Sense of Metrics\n",
    "* “My true positive rate is high, which means that when a POI is present in the test data, I am good at flagging him or her.”\n",
    "* “My identifier doesn’t have great PRECISION, but it does have good RECALL. That means that, nearly every time a POI shows up in my test set, I am able to identify him or her. The cost of this is that I sometimes get some false positives, where non-POIs get flagged.”\n",
    "* “My identifier doesn’t have great RECALL, but it does have good PRECISION. That means that whenever a POI gets flagged in my test set, I know with a lot of confidence that it’s very likely to be a real POI and not a false alarm. On the other hand, the price I pay for this is that I sometimes miss real POIs, since I’m effectively reluctant to pull the trigger on edge cases.”\n",
    "* “My identifier has a really great F1 SCORE. This is the best of both worlds. Both my false positive and false negative rates are LOW, which means that I can identify POI’s reliably and accurately. If my identifier finds a POI then the person is almost certainly a POI, and if the identifier does not flag someone, then they are almost certainly not a POI.”\n",
    "\n",
    "#### There’s usually a tradeoff between precision and recall--which one do you think is more important in your POI identifier? There’s no right or wrong answer, there are good arguments either way, but you should be able to interpret both metrics and articulate which one you find most important and why."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
