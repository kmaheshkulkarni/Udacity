{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify Fraud from Enron Email\n",
    "In this project, you will play detective, and put your machine learning skills to use by building an algorithm to identify Enron Employees who may have committed fraud based on the public Enron financial and email dataset.\n",
    "\n",
    "## Section 1\n",
    "* Summarize for us the goal of this project and how machine learning is useful in trying to accomplish it. As part of your answer, give some background on the dataset and how it can be used to answer the project question. Were there any outliers in the data when you got it, and how did you handle those?  [relevant rubric items: “data exploration”, “outlier investigation”]\n",
    "\n",
    "Enron Corporation was an American energy, commodities, and services company based in Houston, Texas. [https://en.wikipedia.org/wiki/Enron] Enron employed approximately 20,000 staff and was one of the world's major electricity, natural gas, communications and pulp and paper companies, with claimed revenues of nearly $101 billion during 2000. At the end of 2001, it was revealed that its reported financial condition was sustained by institutionalized, systematic, and creatively planned accounting fraud, known since as the Enron scandal. Enron has since become a well-known example of willful corporate fraud and corruption.\n",
    "\n",
    "The goal of this project is to identify persons of interest (poi), which consist of:\n",
    "* Indicted persons\n",
    "* Reached a settlement or plea deal with the government\n",
    "* Testified in exchange for prosecution immunity\n",
    "\n",
    "Machine learning is an advance tool for classification tasks such as the Enron project. We can recognize patterns and regularities in our data and then make predictions or decisions, through the building of models from the sample inputs.\n",
    "\n",
    "### Enron dataset cleaning and EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/perseas/anaconda/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pickle\n",
    "sys.path.append(\"./tools/\")\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data\n",
    "\n",
    "### Task 1: Select what features you'll use.\n",
    "### features_list is a list of strings, each of which is a feature name.\n",
    "### The first feature must be \"poi\".\n",
    "features_list = ['poi',\n",
    "                 'bonus',\n",
    "                 'deferral_payments',\n",
    "                 'deferred_income',\n",
    "                 'director_fees',\n",
    "                 'exercised_stock_options',\n",
    "                 'expenses',\n",
    "                 'from_messages',\n",
    "                 'from_poi_to_this_person',\n",
    "                 'from_this_person_to_poi',\n",
    "                 'loan_advances',\n",
    "                 'long_term_incentive',\n",
    "                 'other',\n",
    "                 'restricted_stock',\n",
    "                 'restricted_stock_deferred',\n",
    "                 'salary',\n",
    "                 'shared_receipt_with_poi',\n",
    "                 'to_messages',\n",
    "                 'total_payments',\n",
    "                 'total_stock_value',\n",
    "                 'fraction_from_poi',\n",
    "                 'fraction_to_poi']\n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"rb\") as data_file:\n",
    "    data_dict = pickle.load(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate dataset\n",
    "Load data into a pandas dataframe for easier exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(146, 21)\n",
      "salary                       object\n",
      "to_messages                  object\n",
      "deferral_payments            object\n",
      "total_payments               object\n",
      "loan_advances                object\n",
      "bonus                        object\n",
      "email_address                object\n",
      "restricted_stock_deferred    object\n",
      "deferred_income              object\n",
      "total_stock_value            object\n",
      "expenses                     object\n",
      "from_poi_to_this_person      object\n",
      "exercised_stock_options      object\n",
      "from_messages                object\n",
      "other                        object\n",
      "from_this_person_to_poi      object\n",
      "poi                            bool\n",
      "long_term_incentive          object\n",
      "shared_receipt_with_poi      object\n",
      "restricted_stock             object\n",
      "director_fees                object\n",
      "dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>salary</th>\n",
       "      <th>to_messages</th>\n",
       "      <th>deferral_payments</th>\n",
       "      <th>total_payments</th>\n",
       "      <th>loan_advances</th>\n",
       "      <th>bonus</th>\n",
       "      <th>email_address</th>\n",
       "      <th>restricted_stock_deferred</th>\n",
       "      <th>deferred_income</th>\n",
       "      <th>total_stock_value</th>\n",
       "      <th>...</th>\n",
       "      <th>from_poi_to_this_person</th>\n",
       "      <th>exercised_stock_options</th>\n",
       "      <th>from_messages</th>\n",
       "      <th>other</th>\n",
       "      <th>from_this_person_to_poi</th>\n",
       "      <th>poi</th>\n",
       "      <th>long_term_incentive</th>\n",
       "      <th>shared_receipt_with_poi</th>\n",
       "      <th>restricted_stock</th>\n",
       "      <th>director_fees</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ALLEN PHILLIP K</th>\n",
       "      <td>201955</td>\n",
       "      <td>2902</td>\n",
       "      <td>2869717</td>\n",
       "      <td>4484442</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4175000</td>\n",
       "      <td>phillip.allen@enron.com</td>\n",
       "      <td>-126027</td>\n",
       "      <td>-3081055</td>\n",
       "      <td>1729541</td>\n",
       "      <td>...</td>\n",
       "      <td>47</td>\n",
       "      <td>1729541</td>\n",
       "      <td>2195</td>\n",
       "      <td>152</td>\n",
       "      <td>65</td>\n",
       "      <td>False</td>\n",
       "      <td>304805</td>\n",
       "      <td>1407</td>\n",
       "      <td>126027</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BADUM JAMES P</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>178980</td>\n",
       "      <td>182466</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>257817</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>257817</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BANNANTINE JAMES M</th>\n",
       "      <td>477</td>\n",
       "      <td>566</td>\n",
       "      <td>NaN</td>\n",
       "      <td>916197</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>james.bannantine@enron.com</td>\n",
       "      <td>-560222</td>\n",
       "      <td>-5104</td>\n",
       "      <td>5243487</td>\n",
       "      <td>...</td>\n",
       "      <td>39</td>\n",
       "      <td>4046157</td>\n",
       "      <td>29</td>\n",
       "      <td>864523</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>465</td>\n",
       "      <td>1757552</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BAXTER JOHN C</th>\n",
       "      <td>267102</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1295738</td>\n",
       "      <td>5634343</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1386055</td>\n",
       "      <td>10623258</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6680544</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2660303</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>1586055</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3942714</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BAY FRANKLIN R</th>\n",
       "      <td>239671</td>\n",
       "      <td>NaN</td>\n",
       "      <td>260455</td>\n",
       "      <td>827696</td>\n",
       "      <td>NaN</td>\n",
       "      <td>400000</td>\n",
       "      <td>frank.bay@enron.com</td>\n",
       "      <td>-82782</td>\n",
       "      <td>-201641</td>\n",
       "      <td>63014</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>69</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>145796</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BAZELIDES PHILIP J</th>\n",
       "      <td>80818</td>\n",
       "      <td>NaN</td>\n",
       "      <td>684694</td>\n",
       "      <td>860136</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1599641</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1599641</td>\n",
       "      <td>NaN</td>\n",
       "      <td>874</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>93750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BECK SALLY W</th>\n",
       "      <td>231330</td>\n",
       "      <td>7315</td>\n",
       "      <td>NaN</td>\n",
       "      <td>969068</td>\n",
       "      <td>NaN</td>\n",
       "      <td>700000</td>\n",
       "      <td>sally.beck@enron.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>126027</td>\n",
       "      <td>...</td>\n",
       "      <td>144</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4343</td>\n",
       "      <td>566</td>\n",
       "      <td>386</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2639</td>\n",
       "      <td>126027</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BELDEN TIMOTHY N</th>\n",
       "      <td>213999</td>\n",
       "      <td>7991</td>\n",
       "      <td>2144013</td>\n",
       "      <td>5501630</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5249999</td>\n",
       "      <td>tim.belden@enron.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2334434</td>\n",
       "      <td>1110705</td>\n",
       "      <td>...</td>\n",
       "      <td>228</td>\n",
       "      <td>953136</td>\n",
       "      <td>484</td>\n",
       "      <td>210698</td>\n",
       "      <td>108</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5521</td>\n",
       "      <td>157569</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BELFER ROBERT</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-102500</td>\n",
       "      <td>102500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>44093</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-44093</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3285</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BERBERIAN DAVID</th>\n",
       "      <td>216582</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>228474</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>david.berberian@enron.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2493616</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1624396</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>869220</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    salary to_messages deferral_payments total_payments  \\\n",
       "ALLEN PHILLIP K     201955        2902           2869717        4484442   \n",
       "BADUM JAMES P          NaN         NaN            178980         182466   \n",
       "BANNANTINE JAMES M     477         566               NaN         916197   \n",
       "BAXTER JOHN C       267102         NaN           1295738        5634343   \n",
       "BAY FRANKLIN R      239671         NaN            260455         827696   \n",
       "BAZELIDES PHILIP J   80818         NaN            684694         860136   \n",
       "BECK SALLY W        231330        7315               NaN         969068   \n",
       "BELDEN TIMOTHY N    213999        7991           2144013        5501630   \n",
       "BELFER ROBERT          NaN         NaN           -102500         102500   \n",
       "BERBERIAN DAVID     216582         NaN               NaN         228474   \n",
       "\n",
       "                   loan_advances    bonus               email_address  \\\n",
       "ALLEN PHILLIP K              NaN  4175000     phillip.allen@enron.com   \n",
       "BADUM JAMES P                NaN      NaN                         NaN   \n",
       "BANNANTINE JAMES M           NaN      NaN  james.bannantine@enron.com   \n",
       "BAXTER JOHN C                NaN  1200000                         NaN   \n",
       "BAY FRANKLIN R               NaN   400000         frank.bay@enron.com   \n",
       "BAZELIDES PHILIP J           NaN      NaN                         NaN   \n",
       "BECK SALLY W                 NaN   700000        sally.beck@enron.com   \n",
       "BELDEN TIMOTHY N             NaN  5249999        tim.belden@enron.com   \n",
       "BELFER ROBERT                NaN      NaN                         NaN   \n",
       "BERBERIAN DAVID              NaN      NaN   david.berberian@enron.com   \n",
       "\n",
       "                   restricted_stock_deferred deferred_income  \\\n",
       "ALLEN PHILLIP K                      -126027        -3081055   \n",
       "BADUM JAMES P                            NaN             NaN   \n",
       "BANNANTINE JAMES M                   -560222           -5104   \n",
       "BAXTER JOHN C                            NaN        -1386055   \n",
       "BAY FRANKLIN R                        -82782         -201641   \n",
       "BAZELIDES PHILIP J                       NaN             NaN   \n",
       "BECK SALLY W                             NaN             NaN   \n",
       "BELDEN TIMOTHY N                         NaN        -2334434   \n",
       "BELFER ROBERT                          44093             NaN   \n",
       "BERBERIAN DAVID                          NaN             NaN   \n",
       "\n",
       "                   total_stock_value      ...      from_poi_to_this_person  \\\n",
       "ALLEN PHILLIP K              1729541      ...                           47   \n",
       "BADUM JAMES P                 257817      ...                          NaN   \n",
       "BANNANTINE JAMES M           5243487      ...                           39   \n",
       "BAXTER JOHN C               10623258      ...                          NaN   \n",
       "BAY FRANKLIN R                 63014      ...                          NaN   \n",
       "BAZELIDES PHILIP J           1599641      ...                          NaN   \n",
       "BECK SALLY W                  126027      ...                          144   \n",
       "BELDEN TIMOTHY N             1110705      ...                          228   \n",
       "BELFER ROBERT                 -44093      ...                          NaN   \n",
       "BERBERIAN DAVID              2493616      ...                          NaN   \n",
       "\n",
       "                   exercised_stock_options from_messages    other  \\\n",
       "ALLEN PHILLIP K                    1729541          2195      152   \n",
       "BADUM JAMES P                       257817           NaN      NaN   \n",
       "BANNANTINE JAMES M                 4046157            29   864523   \n",
       "BAXTER JOHN C                      6680544           NaN  2660303   \n",
       "BAY FRANKLIN R                         NaN           NaN       69   \n",
       "BAZELIDES PHILIP J                 1599641           NaN      874   \n",
       "BECK SALLY W                           NaN          4343      566   \n",
       "BELDEN TIMOTHY N                    953136           484   210698   \n",
       "BELFER ROBERT                         3285           NaN      NaN   \n",
       "BERBERIAN DAVID                    1624396           NaN      NaN   \n",
       "\n",
       "                   from_this_person_to_poi    poi  long_term_incentive  \\\n",
       "ALLEN PHILLIP K                         65  False               304805   \n",
       "BADUM JAMES P                          NaN  False                  NaN   \n",
       "BANNANTINE JAMES M                       0  False                  NaN   \n",
       "BAXTER JOHN C                          NaN  False              1586055   \n",
       "BAY FRANKLIN R                         NaN  False                  NaN   \n",
       "BAZELIDES PHILIP J                     NaN  False                93750   \n",
       "BECK SALLY W                           386  False                  NaN   \n",
       "BELDEN TIMOTHY N                       108   True                  NaN   \n",
       "BELFER ROBERT                          NaN  False                  NaN   \n",
       "BERBERIAN DAVID                        NaN  False                  NaN   \n",
       "\n",
       "                   shared_receipt_with_poi restricted_stock director_fees  \n",
       "ALLEN PHILLIP K                       1407           126027           NaN  \n",
       "BADUM JAMES P                          NaN              NaN           NaN  \n",
       "BANNANTINE JAMES M                     465          1757552           NaN  \n",
       "BAXTER JOHN C                          NaN          3942714           NaN  \n",
       "BAY FRANKLIN R                         NaN           145796           NaN  \n",
       "BAZELIDES PHILIP J                     NaN              NaN           NaN  \n",
       "BECK SALLY W                          2639           126027           NaN  \n",
       "BELDEN TIMOTHY N                      5521           157569           NaN  \n",
       "BELFER ROBERT                          NaN              NaN          3285  \n",
       "BERBERIAN DAVID                        NaN           869220           NaN  \n",
       "\n",
       "[10 rows x 21 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "enron_df = pd.DataFrame.from_dict(data_dict, orient='index')\n",
    "print(enron_df.shape)\n",
    "print(enron_df.dtypes)\n",
    "enron_df.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reformat 'NaN' to numpy nan\n",
    "There are many 'NaN' values which should be reformated to numpy nan and then check again the data types of the Enron dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sum of the nan values: \n",
      "salary                        51\n",
      "to_messages                   60\n",
      "deferral_payments            107\n",
      "total_payments                21\n",
      "loan_advances                142\n",
      "bonus                         64\n",
      "email_address                 35\n",
      "restricted_stock_deferred    128\n",
      "deferred_income               97\n",
      "total_stock_value             20\n",
      "expenses                      51\n",
      "from_poi_to_this_person       60\n",
      "exercised_stock_options       44\n",
      "from_messages                 60\n",
      "other                         53\n",
      "from_this_person_to_poi       60\n",
      "poi                            0\n",
      "long_term_incentive           80\n",
      "shared_receipt_with_poi       60\n",
      "restricted_stock              36\n",
      "director_fees                129\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "salary                       float64\n",
       "to_messages                  float64\n",
       "deferral_payments            float64\n",
       "total_payments               float64\n",
       "loan_advances                float64\n",
       "bonus                        float64\n",
       "email_address                 object\n",
       "restricted_stock_deferred    float64\n",
       "deferred_income              float64\n",
       "total_stock_value            float64\n",
       "expenses                     float64\n",
       "from_poi_to_this_person      float64\n",
       "exercised_stock_options      float64\n",
       "from_messages                float64\n",
       "other                        float64\n",
       "from_this_person_to_poi      float64\n",
       "poi                             bool\n",
       "long_term_incentive          float64\n",
       "shared_receipt_with_poi      float64\n",
       "restricted_stock             float64\n",
       "director_fees                float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Convert NaN to numpy nan\n",
    "enron_df.replace(to_replace='NaN', value=np.nan, inplace=True)\n",
    "\n",
    "print(\"The sum of the nan values: \\n{}\".format(enron_df.isnull().sum()))\n",
    "enron_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The missing values of the financial features are considered zero values and will be replaced later with 0s. As it was noted in the post:\n",
    "\n",
    "https://discussions.udacity.com/t/how-to-start-the-final-project/177617/6\n",
    "\n",
    "the financial feature values were extracted from the \"enron61702insiderpay.pdf\" document. Cells in the table marked as \"-\" are parsed as NaN values in the saved data dictionary, but actually represent values of 0. The conversion of NaNs to 0s is performed when the data is converted from a dictionary to a matrix through the featureFormat() function.\n",
    "\n",
    "However, same does not apply for the email features. Missing values are probably reflective of a true lack of information about the person in the email corpus. Thus, considerations on treatment of email features based on missing values should be different than the treatment of financial features. For the simplicity of the project I will leave them as it is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify the higher salary\n",
    "The bigger salary is the 'TOTAL', which is the sum of all the salaries of the listed Enron employees, so it is definetely an outlier and it will be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL 26704229.0\n"
     ]
    }
   ],
   "source": [
    "print(enron_df['salary'].idxmax(), enron_df['salary'].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POIs:  18\n",
      "Non POIs:  128\n"
     ]
    }
   ],
   "source": [
    "total = enron_df['poi'].count()\n",
    "pois = enron_df[enron_df['poi'] == True]['poi'].count()\n",
    "print(\"POIs: \", pois)\n",
    "print(\"Non POIs: \", total - pois)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enron employees exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ALLEN PHILLIP K', 'BADUM JAMES P', 'BANNANTINE JAMES M',\n",
       "       'BAXTER JOHN C', 'BAY FRANKLIN R', 'BAZELIDES PHILIP J',\n",
       "       'BECK SALLY W', 'BELDEN TIMOTHY N', 'BELFER ROBERT',\n",
       "       'BERBERIAN DAVID', 'BERGSIEKER RICHARD P', 'BHATNAGAR SANJAY',\n",
       "       'BIBI PHILIPPE A', 'BLACHMAN JEREMY M', 'BLAKE JR. NORMAN P',\n",
       "       'BOWEN JR RAYMOND M', 'BROWN MICHAEL', 'BUCHANAN HAROLD G',\n",
       "       'BUTTS ROBERT H', 'BUY RICHARD B', 'CALGER CHRISTOPHER F',\n",
       "       'CARTER REBECCA C', 'CAUSEY RICHARD A', 'CHAN RONNIE',\n",
       "       'CHRISTODOULOU DIOMEDES', 'CLINE KENNETH W', 'COLWELL WESLEY',\n",
       "       'CORDES WILLIAM R', 'COX DAVID', 'CUMBERLAND MICHAEL S',\n",
       "       'DEFFNER JOSEPH M', 'DELAINEY DAVID W', 'DERRICK JR. JAMES V',\n",
       "       'DETMERING TIMOTHY J', 'DIETRICH JANET R', 'DIMICHELE RICHARD G',\n",
       "       'DODSON KEITH', 'DONAHUE JR JEFFREY M', 'DUNCAN JOHN H',\n",
       "       'DURAN WILLIAM D', 'ECHOLS JOHN B', 'ELLIOTT STEVEN',\n",
       "       'FALLON JAMES B', 'FASTOW ANDREW S', 'FITZGERALD JAY L',\n",
       "       'FOWLER PEGGY', 'FOY JOE', 'FREVERT MARK A', 'FUGH JOHN L',\n",
       "       'GAHN ROBERT S', 'GARLAND C KEVIN', 'GATHMANN WILLIAM D',\n",
       "       'GIBBS DANA R', 'GILLIS JOHN', 'GLISAN JR BEN F', 'GOLD JOSEPH',\n",
       "       'GRAMM WENDY L', 'GRAY RODNEY', 'HAEDICKE MARK E', 'HANNON KEVIN P',\n",
       "       'HAUG DAVID L', 'HAYES ROBERT E', 'HAYSLETT RODERICK J',\n",
       "       'HERMANN ROBERT J', 'HICKERSON GARY J', 'HIRKO JOSEPH',\n",
       "       'HORTON STANLEY C', 'HUGHES JAMES A', 'HUMPHREY GENE E',\n",
       "       'IZZO LAWRENCE L', 'JACKSON CHARLENE R', 'JAEDICKE ROBERT',\n",
       "       'KAMINSKI WINCENTY J', 'KEAN STEVEN J', 'KISHKILL JOSEPH G',\n",
       "       'KITCHEN LOUISE', 'KOENIG MARK E', 'KOPPER MICHAEL J',\n",
       "       'LAVORATO JOHN J', 'LAY KENNETH L', 'LEFF DANIEL P',\n",
       "       'LEMAISTRE CHARLES', 'LEWIS RICHARD', 'LINDHOLM TOD A',\n",
       "       'LOCKHART EUGENE E', 'LOWRY CHARLES P', 'MARTIN AMANDA K',\n",
       "       'MCCARTY DANNY J', 'MCCLELLAN GEORGE', 'MCCONNELL MICHAEL S',\n",
       "       'MCDONALD REBECCA', 'MCMAHON JEFFREY', 'MENDELSOHN JOHN',\n",
       "       'METTS MARK', 'MEYER JEROME J', 'MEYER ROCKFORD G',\n",
       "       'MORAN MICHAEL P', 'MORDAUNT KRISTINA M', 'MULLER MARK S',\n",
       "       'MURRAY JULIA H', 'NOLES JAMES L', 'OLSON CINDY K',\n",
       "       'OVERDYKE JR JERE C', 'PAI LOU L', 'PEREIRA PAULO V. FERRAZ',\n",
       "       'PICKERING MARK R', 'PIPER GREGORY F', 'PIRO JIM', 'POWERS WILLIAM',\n",
       "       'PRENTICE JAMES', 'REDMOND BRIAN L', 'REYNOLDS LAWRENCE',\n",
       "       'RICE KENNETH D', 'RIEKER PAULA H', 'SAVAGE FRANK',\n",
       "       'SCRIMSHAW MATTHEW', 'SHANKMAN JEFFREY A', 'SHAPIRO RICHARD S',\n",
       "       'SHARP VICTORIA T', 'SHELBY REX', 'SHERRICK JEFFREY B',\n",
       "       'SHERRIFF JOHN R', 'SKILLING JEFFREY K', 'STABLER FRANK',\n",
       "       'SULLIVAN-SHAKLOVITZ COLLEEN', 'SUNDE MARTIN', 'TAYLOR MITCHELL S',\n",
       "       'THE TRAVEL AGENCY IN THE PARK', 'THORN TERENCE H',\n",
       "       'TILNEY ELIZABETH A', 'TOTAL', 'UMANOFF ADAM S', 'URQUHART JOHN A',\n",
       "       'WAKEHAM JOHN', 'WALLS JR ROBERT H', 'WALTERS GARETH W',\n",
       "       'WASAFF GEORGE', 'WESTFAHL RICHARD K', 'WHALEY DAVID A',\n",
       "       'WHALLEY LAWRENCE G', 'WHITE JR THOMAS E', 'WINOKUR JR. HERBERT S',\n",
       "       'WODRASKA JOHN', 'WROBEL BRUCE', 'YEAGER F SCOTT', 'YEAP SOON'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enron_df.index.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove outliers\n",
    "The higher salary of 'TOTAL', as well as 'THE TRAVEL AGENCY IN THE PARK', which was revealed from the previous exploration of the list of Enron Employees will be removed, since they don't belong to real persons. The graphical representation of bonus vs salary depicts some really high values but not unreasonable to pop them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x118e48550>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaIAAAEKCAYAAABQRFHsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEJmZWa7+PxFTUaltAXxQAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x118db2a58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%pylab inline\n",
    "\n",
    "enron_df.drop(['TOTAL', 'THE TRAVEL AGENCY IN THE PARK'], inplace=True)\n",
    "enron_df.plot.scatter(x = 'salary', y = 'bonus')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove outliers from data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bonus': 'NaN',\n",
       " 'deferral_payments': 'NaN',\n",
       " 'deferred_income': 'NaN',\n",
       " 'director_fees': 'NaN',\n",
       " 'email_address': 'NaN',\n",
       " 'exercised_stock_options': 'NaN',\n",
       " 'expenses': 'NaN',\n",
       " 'from_messages': 'NaN',\n",
       " 'from_poi_to_this_person': 'NaN',\n",
       " 'from_this_person_to_poi': 'NaN',\n",
       " 'loan_advances': 'NaN',\n",
       " 'long_term_incentive': 'NaN',\n",
       " 'other': 362096,\n",
       " 'poi': False,\n",
       " 'restricted_stock': 'NaN',\n",
       " 'restricted_stock_deferred': 'NaN',\n",
       " 'salary': 'NaN',\n",
       " 'shared_receipt_with_poi': 'NaN',\n",
       " 'to_messages': 'NaN',\n",
       " 'total_payments': 362096,\n",
       " 'total_stock_value': 'NaN'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Task 2: Remove outliers\n",
    "data_dict.pop('TOTAL')\n",
    "data_dict.pop('THE TRAVEL AGENCY IN THE PARK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Visualise the data\n",
    "Visualising the data will help to understand our data and find any other outliers. There is a nice representation of the Enron data in the following link:\n",
    "\n",
    "https://public.tableau.com/profile/diego2420#!/vizhome/Udacity/UdacityDashboard\n",
    "\n",
    "This dashboard was made by Diego using Tableau and was posted in the Udacity Discussions Forum:\n",
    "\n",
    "https://discussions.udacity.com/t/enron-data-set-visualization/33340"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enron dataset conclusions\n",
    "It appears that:\n",
    "* There are 21 features\n",
    "    * 1 \"poi\" (person of interest) feature\n",
    "    * 6 email features\n",
    "    * 14 financial features\n",
    "* There are 146 Enron employees\n",
    "    * 128 non POIs\n",
    "    * 18 POIs\n",
    "* All the features have missing 'NaN' values except from 'poi' which is a boolean.\n",
    "* Exploring the data, it was found two features, 'TOTAL' and 'THE TRAVEL AGENCY IN THE PARK', which are not persons and was removed.\n",
    "* Some extreme values which belong to persons are not considered outliers and will help the algorithms to identify pois.\n",
    "* The dataset will be susceptible to overfitting as it contains too many features and few data. In the following steps, we will try to apply various techniques to overcome this problem, such as:\n",
    "    * Feature removal\n",
    "    * Feature selection\n",
    "    * Cross-validation\n",
    "\n",
    "Missing values, i.e. 'NaN' strings will be replaced later with 0.0 using the featureFormat() function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Section 2\n",
    "* What features did you end up using in your POI identifier, and what selection process did you use to pick them? Did you have to do any scaling? Why or why not? As part of the assignment, you should attempt to engineer your own feature that does not come ready-made in the dataset -- explain what feature you tried to make, and the rationale behind it. (You do not necessarily have to use it in the final analysis, only engineer and test it.) In your feature selection step, if you used an algorithm like a decision tree, please also give the feature importances of the features that you use, and if you used an automated feature selection function like SelectKBest, please report the feature scores and reasons for your choice of parameter values.  [relevant rubric items: “create new features”, “intelligently select features”, “properly scale features”]\n",
    "\n",
    "### Create new features \n",
    "The number of sent/received emails to/from POIs is an interesting feature to be engineered and investigated. A better robust approach is actually the fraction of these emails to all emails than the absolute number of \"poi emails\":\n",
    "1. fraction_to_poi\n",
    "2. fraction_from_poi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Task 3: Create new feature(s)\n",
    "# Create a function that computes either quantity\n",
    "def computeFraction( poi_messages, all_messages ):\n",
    "    ### take care of \"NaN\" when there is no known email address (and so\n",
    "    ### no filled email features), and integer division.\n",
    "    ### in case of poi_messages or all_messages having \"NaN\" value, return 0.\n",
    "    fraction = 0\n",
    "    if poi_messages == \"NaN\" or all_messages == \"NaN\":\n",
    "        return fraction\n",
    "    else:\n",
    "        fraction = float(poi_messages)/float(all_messages)\n",
    "        \n",
    "    return fraction\n",
    "\n",
    "for name in data_dict:\n",
    "\n",
    "    data_point = data_dict[name]\n",
    "\n",
    "    from_poi_to_this_person = data_point[\"from_poi_to_this_person\"]\n",
    "    to_messages = data_point[\"to_messages\"]\n",
    "    fraction_from_poi = computeFraction( from_poi_to_this_person, to_messages )\n",
    "    data_point[\"fraction_from_poi\"] = fraction_from_poi\n",
    "\n",
    "    from_this_person_to_poi = data_point[\"from_this_person_to_poi\"]\n",
    "    from_messages = data_point[\"from_messages\"]\n",
    "    fraction_to_poi = computeFraction( from_this_person_to_poi, from_messages )\n",
    "    data_point[\"fraction_to_poi\"] = fraction_to_poi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select features\n",
    "SelectKBest was used as a univariate feature selection which works by selecting the best features based on univariate statistical tests. SelectKBest removes all but the k highest scoring features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exercised_stock_options 24.8150797332\n",
      "total_stock_value 24.1828986786\n",
      "bonus 20.7922520472\n",
      "salary 18.2896840434\n",
      "fraction_to_poi 16.409712548\n",
      "deferred_income 11.4584765793\n",
      "long_term_incentive 9.92218601319\n",
      "restricted_stock 9.21281062198\n",
      "total_payments 8.77277773009\n",
      "shared_receipt_with_poi 8.58942073168\n",
      "loan_advances 7.18405565829\n",
      "expenses 6.09417331064\n",
      "from_poi_to_this_person 5.24344971337\n",
      "other 4.187477507\n",
      "fraction_from_poi 3.12809174816\n",
      "from_this_person_to_poi 2.38261210823\n",
      "director_fees 2.12632780201\n",
      "to_messages 1.64634112944\n",
      "deferral_payments 0.224611274736\n",
      "from_messages 0.169700947622\n",
      "restricted_stock_deferred 0.0654996529099\n"
     ]
    }
   ],
   "source": [
    "### Store to my_dataset for easy export below.\n",
    "my_dataset = data_dict\n",
    "\n",
    "### Extract features and labels from dataset for local testing\n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "select = SelectKBest(k=10)\n",
    "select.fit(features, labels)\n",
    "scores = select.scores_\n",
    "# Show the scores in a table\n",
    "feature_scores = zip(features_list[1:], scores)\n",
    "ordered_feature_scores = sorted(feature_scores, key=lambda x: x[1], reverse=True)\n",
    "for feature, score in ordered_feature_scores:\n",
    "    print(feature, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table above shows all the features with their respective scores. The engineered feature \"fraction_to_poi\" is one of the most important features taking the fifth place, behind the \"exercised_stock_options\", \"total_stock_value\", \"bonus\" and \"salary\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale features\n",
    "The next essential preprocessing step after feature engineering and feature selection is to scale all the features applying the \"MinMaxScaler\" method. Since the range of values of our data varies widely (several orders of magnitude), the machine learning algorithm functions will not work properly without normalization. If a feature has a variance that is orders of magnitude larger than others, it might dominate the objective function and make the estimator unable to learn from other features correctly as expected. Therefore, the range of all features should be normalized so that each feature contributes approximately proportionately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "features = scaler.fit_transform(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3-6\n",
    "* What algorithm did you end up using? What other one(s) did you try? How did model performance differ between algorithms?  [relevant rubric item: “pick an algorithm”]\n",
    "* What does it mean to tune the parameters of an algorithm, and what can happen if you don’t do this well?  How did you tune the parameters of your particular algorithm? What parameters did you tune? (Some algorithms do not have parameters that you need to tune -- if this is the case for the one you picked, identify and briefly explain how you would have done it for the model that was not your final choice or a different model that does utilize parameter tuning, e.g. a decision tree classifier).  [relevant rubric items: “discuss parameter tuning”, “tune the algorithm”]\n",
    "* What is validation, and what’s a classic mistake you can make if you do it wrong? How did you validate your analysis?  [relevant rubric items: “discuss validation”, “validation strategy”]\n",
    "* Give at least 2 evaluation metrics and your average performance for each of them.  Explain an interpretation of your metrics that says something human-understandable about your algorithm’s performance. [relevant rubric item: “usage of evaluation metrics”]\n",
    "\n",
    "### Test algorithms\n",
    "After creating new features, selecting the best features and scaling them, the next natural step is to run the algorithms to see how they perform after these preprocess operations. A useful scikit-learn tool, \"Pipeline\", packages the transformation steps of these operations with the estimation step of an algorithm classifier into a coherent workflow. The reasons to use \"Pipeline\" instead of keeping the steps separate are [https://www.civisanalytics.com/blog/workflows-in-python-using-pipeline-and-gridsearchcv-for-more-compact-and-comprehensive-code/]:\n",
    "\n",
    "1. It makes code more readable (or, if you like, it makes the intent of the code clearer).\n",
    "2. We don’t have to worry about keeping track data during intermediate steps, for example between transforming and estimating.\n",
    "3. It makes it trivial to move ordering of the pipeline pieces, or to swap pieces in and out.\n",
    "4. It allows you to do GridSearchCV on your workflow.\n",
    "\n",
    "### Tune the algorithms\n",
    "Tuning an algorithm is a process in which we optimize the parameters that impact the model in order to enable the algorithm to perform with an improved performance. If we don't tune the algorithms well, performance will be poor with low accuracy, precision or reacall. To make it more concrete, for example in the KNN algorithm, we must specify the number of K's in our model (or centroids), that are used. To find the number which gives the best performance of KNN, we have to try several different values of K for a model.\n",
    "\n",
    "Trying lots of values for each of these free parameters is tedious, and there can sometimes be interactions between the choices we make in one step and the optimal value for a downstream step. Hopefully, there are two simple and easy tuning strategies, grid search and random search. Scikit-learn provides these two methods for algorithm parameter tuning. GridSearchCV allows us to construct a grid of all the combinations of parameters passing one classifier to pipeline each time, tries each combination, and then reports back the best combination. So, instead of trying numerous values for each tuning parameter, GridSearchCV will apply all the combinations of parameters - not just vary them independently - avoiding local optima.\n",
    "\n",
    "To summarize, the power of GridSearchCV is that it multiplies out all the combinations of parameters and tries each one, making a k-fold cross-validated model for each combination. Then, we can ask for predictions and parameters from our GridSearchCV object and it will automatically return to us the best set of predictions, as well as the best parameters.\n",
    "\n",
    "The following algoriths were studied:\n",
    "* Naive Bayes\n",
    "* Support Vector Machines\n",
    "* Decision Trees\n",
    "* K Nearest Neighbors\n",
    "* Random Forest\n",
    "* AdaBoost\n",
    "\n",
    "### Validate analysis\n",
    "The process of measuring the effectiveness of an algorithm for every possible input or how well it is fit and it generalizes to new data is called validation. A common mistake is to train and test the algorithm on the same data, which results to over-fitting, i.e. algorithm performs great on the training dataset, but it suffers on new data. To overcome this, data are separated into training and testing tests. In our analysis two validation steps applied through pipeline:\n",
    "1. Data was separated into 70% training and 30% testing sets.\n",
    "2. Because of the small size of the Enron dataset, stratified shuffle split method was used, which returns multiple stratified randomized splits enhancing cross validation performance. The same strategy is used in tester.py with 1000 folds, while 10 iterations were used for the initial algorithms exploration.\n",
    "\n",
    "### Evaluate the algorithms\n",
    "The fraction of correct predictions, i.e. accuracy is typically not enough information to evaluate a model. Although it is a starting point, it can lead to invalid decisions. Models with high accuracy can have terrible precision or recall scores. For this reason the evaluation metrics that were assessed are:\n",
    "1. Precision, the ratio tp / (tp + fp) where tp is the number of true positives and fp the number of false positives. The precision is intuitively the ability of the classifier not to label as positive a sample that is negative. The best value is 1 and the worst value is 0.\n",
    "2. Recall, the ratio tp / (tp + fn) where tp is the number of true positives and fn the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples. The best value is 1 and the worst value is 0.\n",
    "3. F1 score,  a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0. The relative contribution of precision and recall to the F1 score are equal. The formula for the F1 score is: F1 = 2 * (precision * recall) / (precision + recall). Models were evaluated based on this metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########################################################################################################\n",
      "GaussianNB\n",
      "Report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.89      0.89      0.89        38\n",
      "        1.0       0.20      0.20      0.20         5\n",
      "\n",
      "avg / total       0.81      0.81      0.81        43\n",
      "\n",
      "Best f1-score:\n",
      "0.466370851371\n",
      "Best parameters:\n",
      "{'feature_selection__k': 3}\n",
      "Time passed:  0.124 s\n",
      "##########################################################################################################\n",
      "SVM\n",
      "Report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.88      0.97      0.93        38\n",
      "        1.0       0.00      0.00      0.00         5\n",
      "\n",
      "avg / total       0.78      0.86      0.82        43\n",
      "\n",
      "Best f1-score:\n",
      "0.361904761905\n",
      "Best parameters:\n",
      "{'clf__C': 100, 'clf__gamma': 1, 'feature_selection__k': 1}\n",
      "Time passed:  0.594 s\n",
      "##########################################################################################################\n",
      "Decision Trees\n",
      "Report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.88      0.95      0.91        38\n",
      "        1.0       0.00      0.00      0.00         5\n",
      "\n",
      "avg / total       0.78      0.84      0.81        43\n",
      "\n",
      "Best f1-score:\n",
      "0.536688311688\n",
      "Best parameters:\n",
      "{'clf__criterion': 'gini', 'feature_selection__k': 1}\n",
      "Time passed:  0.264 s\n",
      "##########################################################################################################\n",
      "KNN\n",
      "Report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.88      0.95      0.91        38\n",
      "        1.0       0.00      0.00      0.00         5\n",
      "\n",
      "avg / total       0.78      0.84      0.81        43\n",
      "\n",
      "Best f1-score:\n",
      "0.435555555556\n",
      "Best parameters:\n",
      "{'clf__algorithm': 'auto', 'clf__n_neighbors': 3, 'feature_selection__k': 1}\n",
      "Time passed:  1.789 s\n",
      "##########################################################################################################\n",
      "Random Forest\n",
      "Report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.88      0.95      0.91        38\n",
      "        1.0       0.00      0.00      0.00         5\n",
      "\n",
      "avg / total       0.78      0.84      0.81        43\n",
      "\n",
      "Best f1-score:\n",
      "0.537619047619\n",
      "Best parameters:\n",
      "{'clf__criterion': 'gini', 'clf__n_estimators': 10, 'feature_selection__k': 1}\n",
      "Time passed:  2.763 s\n",
      "##########################################################################################################\n",
      "AdaBoost\n",
      "Report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.90      0.92      0.91        38\n",
      "        1.0       0.25      0.20      0.22         5\n",
      "\n",
      "avg / total       0.82      0.84      0.83        43\n",
      "\n",
      "Best f1-score:\n",
      "0.536688311688\n",
      "Best parameters:\n",
      "{'clf__n_estimators': 45, 'feature_selection__k': 1}\n",
      "Time passed:  8.651 s\n"
     ]
    }
   ],
   "source": [
    "import warnings; warnings.simplefilter('ignore')\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.metrics import classification_report\n",
    "from time import time\n",
    "\n",
    "features_train, features_test, labels_train, labels_test = \\\n",
    "    train_test_split(features, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "# Instantiate the pipeline steps\n",
    "select = SelectKBest()\n",
    "scaler = MinMaxScaler()\n",
    "nb = GaussianNB()\n",
    "svc = SVC()\n",
    "dtc = DecisionTreeClassifier()\n",
    "knc = KNeighborsClassifier()\n",
    "rfc = RandomForestClassifier()\n",
    "abc = AdaBoostClassifier()\n",
    "\n",
    "# Make a dictionary of classifiers\n",
    "classifiers = {\"GaussianNB\": nb, \"SVM\": svc, \"Decision Trees\": dtc, \n",
    "               \"KNN\": knc, \"Random Forest\": rfc, \"AdaBoost\": abc}\n",
    "\n",
    "# Create a function that combines pipeline and grid search and returns the best clf with the best parameters\n",
    "def optimize_clf(clf, param_grid, n_splits):\n",
    "    t0 = time()\n",
    "    # Add pipeline steps into a list\n",
    "    steps = [('feature_selection', select),\n",
    "             ('feature_scaling', scaler),\n",
    "             ('clf', clf)]\n",
    "    \n",
    "    # Create the pipeline\n",
    "    pipeline = Pipeline(steps)\n",
    "    \n",
    "    # Create Stratified ShuffleSplit cross-validator.\n",
    "    # Provides train/test indices to split data in train/test sets.\n",
    "    sss = StratifiedShuffleSplit(n_splits=n_splits, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Create grid search\n",
    "    cv = GridSearchCV(pipeline, param_grid, scoring='f1', cv=sss)\n",
    "                \n",
    "    # Fit pipeline on features_train and labels_train\n",
    "    cv.fit(features_train, labels_train)\n",
    "    # Call pipeline.predict() on features_test data to make a set of test predictions\n",
    "    labels_pred = cv.predict(features_test)\n",
    "    # Test predictions using sklearn.classification_report()\n",
    "    report = classification_report(labels_test, labels_pred)\n",
    "    # Find the best parameters and scores\n",
    "    best_params = cv.best_params_\n",
    "    best_score = cv.best_score_\n",
    "    # Print the reports\n",
    "    print(\"Report:\")\n",
    "    print(report)\n",
    "    print(\"Best f1-score:\")\n",
    "    print(best_score)\n",
    "    print(\"Best parameters:\")\n",
    "    print(best_params)\n",
    "    print(\"Time passed: \", round(time() - t0, 3), \"s\")\n",
    "    # Return the best estimator\n",
    "    return cv.best_estimator_\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    print(\"##########################################################################################################\")\n",
    "    print(name)\n",
    "    if clf == nb:\n",
    "        parameters = {'feature_selection__k':[1, 3, 5]}\n",
    "    elif clf == svc:\n",
    "        parameters = [{'feature_selection__k':[1, 3, 5],\n",
    "                      'clf__C':[10, 100],\n",
    "                      'clf__gamma':[0.1, 1]}]\n",
    "    elif clf == dtc:\n",
    "        parameters = [{'feature_selection__k':[1, 3, 5],\n",
    "                      'clf__criterion':['gini', 'entropy']}]\n",
    "    elif clf == knc:\n",
    "        parameters = [{'feature_selection__k':[1, 3, 5],\n",
    "                      'clf__n_neighbors':[3, 5, 7],\n",
    "                      'clf__algorithm':['auto', 'ball_tree', 'kd_tree', 'brute']}]\n",
    "    elif clf == rfc:\n",
    "        parameters = [{'feature_selection__k':[1, 3, 5],\n",
    "                      'clf__n_estimators':[1, 5, 10],\n",
    "                      'clf__criterion':['gini', 'entropy']}]\n",
    "    elif clf == abc:\n",
    "        parameters = [{'feature_selection__k':[1, 3, 5],\n",
    "                      'clf__n_estimators':[45, 50, 55]}]\n",
    "    optimize_clf(clf, parameters, n_splits=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tuning\n",
    "After testing and tuning the algorithms, most of them perfoms quite well. The best algorithm in terms of acceptable f1 score and reasonable time cost was found to be the decision trees. AdaBoost and Random Forest perfomed equally or better than Decision Trees, but the required processing time is considerably greater. This is an important factor to consider when we want to run multiple times our algorithms. For this reason, decision trees will be further tuned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.88      0.95      0.91        38\n",
      "        1.0       0.00      0.00      0.00         5\n",
      "\n",
      "avg / total       0.78      0.84      0.81        43\n",
      "\n",
      "Best f1-score:\n",
      "0.536688311688\n",
      "Best parameters:\n",
      "{'clf__class_weight': None, 'clf__criterion': 'gini', 'clf__max_depth': None, 'clf__max_features': None, 'feature_selection__k': 1}\n",
      "Time passed:  23.877 s\n"
     ]
    }
   ],
   "source": [
    "parameters = [{'feature_selection__k':[1, 2, 3, 4, 5],\n",
    "               'clf__criterion':['gini', 'entropy'],\n",
    "               'clf__max_depth':[None, 1, 2, 3, 4],\n",
    "               'clf__max_features':[None, 'auto', 'sqrt', 'log2'],\n",
    "               'clf__class_weight':[None, 'balanced']}]\n",
    "\n",
    "clf = optimize_clf(dtc, parameters, n_splits=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "Keeping only the 5 features with higher importances to prevent from overfitting and using the desicion trees as the algorithm model, it was achieved the desirable scoring for both precision and recall. Run poi_id.py and tester.py to generate the results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
